{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stinson2019 = pd.read_pickle(\"../data/stinson2019/stinson2019_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ids(data):\n",
    "    \"\"\"Add unique record ids for each entry in sap dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        Dataframe containing sap records\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Dataframe same as input but with new column containing a\n",
    "        unique record id for each entry of the form:\n",
    "        \n",
    "        '<TreeID>_<TapID>_<RecordYear>_<ID#>'\n",
    "\n",
    "        where ID# is 0 for the first record for a given tap in <RecordYear>, \n",
    "        1 is the second record of the year, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    id_df = data.sort_values([\"site\", \"tree\", \"tap\", \"date\"])\n",
    "\n",
    "    # Create unique record ids for each entry in the following form:\n",
    "    # \"<TreeID>_<TapID>_<RecordYear>_<ID#>\" where ID# is 0 for the first\n",
    "    # record for a given tap in <RecordYear>, 1 is the second ...\n",
    "\n",
    "    # First create \"<TreeID>_<TapID>_<RecordYear>_\" label for each record_id\n",
    "    id_df[\"record_id\"] = (\n",
    "        +id_df[\"tree\"]\n",
    "        + \"_\"\n",
    "        + id_df[\"tap\"]\n",
    "        + \"_\"\n",
    "        + pd.DatetimeIndex(id_df[\"year\"]).year.astype(str)\n",
    "        + \"_\"\n",
    "    )\n",
    "\n",
    "    # Add \"<ID#>\" to each record_id\n",
    "    for tapyear in id_df[\"record_id\"].unique():\n",
    "        id_df.loc[id_df[\"record_id\"] == tapyear, \"record_id\"] += [\n",
    "            str(i) for i in range(id_df[id_df[\"record_id\"] == tapyear].shape[0])\n",
    "        ]\n",
    "\n",
    "    id_df[\"tap_id\"] = id_df[\"tree\"] + id_df[\"tap\"]\n",
    "    \n",
    "    return id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_tables(data):\n",
    "    \"\"\"Create normalized tables from wide dataframe of sap measurements\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        Wide dataframe of sap measurements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dict of 7 normalized dataframes: tap_records, sap, sugar, dates, tap_tree, tree_species, site\n",
    "    \"\"\"\n",
    "    df = {}\n",
    "\n",
    "    df[\"tap_records\"] = data[[\"record_id\", \"tap_id\"]].set_index(\"record_id\")\n",
    "    df[\"sap\"] = (\n",
    "        data[[\"record_id\", \"sap_wt\"]]\n",
    "        .rename(columns={\"sap_wt\": \"sap\"})\n",
    "        .set_index(\"record_id\")\n",
    "    )\n",
    "    df[\"sugar\"] = data[[\"record_id\", \"sugar\"]].set_index(\"record_id\")\n",
    "    df[\"dates\"] = data[[\"record_id\", \"date\"]].set_index(\"record_id\")\n",
    "    df[\"dates\"].loc[:, \"date\"] = pd.to_datetime(df[\"dates\"][\"date\"])\n",
    "    df[\"tap_tree\"] = data[[\"tap_id\", \"tree\"]].drop_duplicates().set_index(\"tap_id\")\n",
    "    df[\"tree_species\"] = data[[\"tree\", \"species\"]].drop_duplicates().set_index(\"tree\")\n",
    "    df[\"site\"] = data[[\"tree\", \"site\"]].drop_duplicates().set_index(\"tree\")\n",
    "    df[\"site\"][\"site\"] = df['site'][\"site\"].str.upper()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_data(\n",
    "    normalized_data,\n",
    "    location=[\"all\"],\n",
    "    tree=\"all\",\n",
    "    tap_id=\"all\",\n",
    "    years=\"all\",\n",
    "    species=\"ACSA\",\n",
    "):\n",
    "    \"\"\"Generate data table containing cumulative weekly sap and sugar amounts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    normalized_data : list\n",
    "        List of normalized dataframes from `normalized_tables` function.\n",
    "    location : str or list of str, optional\n",
    "        Name of locations (sites) to be included in data table, by default 'all'\n",
    "    tree : str or list of str, optional\n",
    "        ID of trees to be included in data table, by default 'all'\n",
    "    tap_id : str or list of str, optional\n",
    "        ID of taps to be included in data table, by default 'all'\n",
    "    years : int, list of ints, or 'all' , optional\n",
    "        Years to be included in data table, by default 'all'\n",
    "    species : str, list of str, or 'all' , optional\n",
    "        Species to be included in data table, by default 'ACSA' (sugar maple)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Table with weekly summaries for all taps specified in arguments.  Includes\n",
    "        cumulative sap and sugar weight, and weekly sap and sugar weight.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack normalized DataFrames\n",
    "    tap_records, sap, sugar, dates, tap_tree, tree_species, site = normalized_data.values()\n",
    "    \n",
    "    # Check and clean location argument\n",
    "    if type(location) != list:\n",
    "        location = [location]\n",
    "    location = [x.upper() for x in location]\n",
    "    if location == [\"ALL\"]:\n",
    "        location = site[\"site\"].unique().tolist()\n",
    "\n",
    "    # Check and clean tap_id argument\n",
    "    if type(tap_id) != list:\n",
    "        tap_id = [tap_id]\n",
    "    tap_id = [x.upper() for x in tap_id]\n",
    "    if tap_id == [\"ALL\"]:\n",
    "        tap_id = tap_tree.index.tolist()\n",
    "\n",
    "    # Check and clean tree argument\n",
    "    if type(tree) != list:\n",
    "        tree = [tree]\n",
    "    tree = [x.upper() for x in tree]\n",
    "    if tree == [\"ALL\"]:\n",
    "        tree = tap_tree[\"tree\"].unique().tolist()\n",
    "\n",
    "    # Check and clean years argument\n",
    "    if type(years) != list:\n",
    "        years = [years]\n",
    "    if type(years[0]) == str:\n",
    "        years[0] = years[0].upper()\n",
    "        if years == [\"ALL\"]:\n",
    "            years = pd.DatetimeIndex(dates[\"date\"]).year.unique().tolist()\n",
    "\n",
    "    # Check and clean species argument\n",
    "    if type(species) != list:\n",
    "        species = [species]\n",
    "    species = [x.upper() for x in species]\n",
    "    if species == [\"ALL\"]:\n",
    "        species = tree_species[\"species\"].unique().tolist()\n",
    "\n",
    "    tap_id = (\n",
    "        tap_tree[tap_tree.index.isin(tap_id)]\n",
    "        .join(site, how=\"left\", on=\"tree\")\n",
    "        .reset_index()\n",
    "        .merge(tree_species, how=\"left\", on=\"tree\")\n",
    "        .set_index(\"tap_id\")\n",
    "    )\n",
    "    tap_id = tap_id[\n",
    "        (tap_id[\"tree\"].isin(tree))\n",
    "        & (tap_id[\"site\"].isin(location))\n",
    "        & (tap_id[\"species\"].isin(species))\n",
    "    ].index.tolist()\n",
    "\n",
    "    # Initialize summary dataframe\n",
    "    weekly_df = pd.DataFrame()\n",
    "\n",
    "    # Create weekly summaries, iterating through all taps\n",
    "    for tap in tap_id:\n",
    "        #         print(\"tap:  \", tap)\n",
    "        # Create joint dataframe will all required info for current tap\n",
    "        df = (\n",
    "            tap_records.join(tap_tree[tap_tree.index == tap], how=\"right\", on=\"tap_id\")\n",
    "            .join(sap, how=\"left\")\n",
    "            .join(sugar, how=\"left\")\n",
    "            .join(dates[pd.DatetimeIndex(dates[\"date\"]).year.isin(years)], how=\"inner\")\n",
    "        )\n",
    "        df[\"year\"] = pd.DatetimeIndex(df[\"date\"]).year\n",
    "        df[\"jd\"] = pd.DatetimeIndex(df[\"date\"]).dayofyear\n",
    "\n",
    "        for year in df[\"year\"].unique():\n",
    "            #             print('     year: ', year)\n",
    "            df_year = df[df[\"year\"] == year]\n",
    "\n",
    "            # Deal with multiple entries per day.  Sap taken as sum of measurements, sugar content as weighted average.\n",
    "            if not df_year[\"jd\"].is_unique:\n",
    "                df_year_temp = copy.copy(df_year)\n",
    "                df_year_temp[\"product\"] = df_year_temp.sap * df_year_temp.sugar.fillna(\n",
    "                    value=df_year_temp.sugar.mean()\n",
    "                )\n",
    "                df_year_temp = df_year_temp.groupby(by=\"jd\").sum().reset_index()\n",
    "                df_year_temp[\"sugar\"] = df_year_temp[\"product\"] / df_year_temp[\"sap\"]\n",
    "                df_year = df_year.drop_duplicates(subset=\"jd\")\n",
    "                df_year = (\n",
    "                    df_year.reset_index()\n",
    "                    .merge(\n",
    "                        df_year_temp[[\"jd\", \"sap\", \"sugar\"]],\n",
    "                        on=\"jd\",\n",
    "                        how=\"right\",\n",
    "                        suffixes=[\"\", \"_sum\"],\n",
    "                    )\n",
    "                    .set_index(\"record_id\")\n",
    "                )\n",
    "                df_year[\"sap\"] = df_year[\"sap_sum\"]\n",
    "                df_year[\"sugar\"] = df_year[\"sugar_sum\"]\n",
    "                df_year = df_year.drop(columns=[\"sap_sum\", \"sugar_sum\"])\n",
    "\n",
    "            # Add entry for every day of year from first day with recorded flow to last\n",
    "            df_year = (\n",
    "                df_year.reset_index()\n",
    "                .merge(\n",
    "                    pd.date_range(\n",
    "                        start=df_year[\"date\"].min(), end=df_year[\"date\"].max()\n",
    "                    ).to_frame(name=\"date\"),\n",
    "                    how=\"right\",\n",
    "                    on=\"date\",\n",
    "                )\n",
    "                .set_index(\"date\", drop=False)\n",
    "            )\n",
    "\n",
    "            # Assumption: missing sugar content should be filled with mean sugar content\n",
    "            df_year[\"sugarwt\"] = (\n",
    "                df_year.sap * df_year.sugar.fillna(value=df_year.sugar.mean()) / 100\n",
    "            )\n",
    "\n",
    "            # Assumption: missing sap values should be replaced with zeros\n",
    "            df_year[\"cum_sap\"] = df_year.sap.fillna(value=0).cumsum()\n",
    "            df_year[\"cum_sugarwt\"] = df_year.sugarwt.fillna(value=0).cumsum()\n",
    "            df_year[\"tap_id\"] = df_year.tap_id.fillna(value=tap)\n",
    "            df_year[\"tree\"] = df_year.tree.fillna(value=df_year.tree[0])\n",
    "            df_year[\"year\"] = pd.DatetimeIndex(df_year[\"date\"]).year\n",
    "            df_year[\"jd\"] = pd.DatetimeIndex(df_year[\"date\"]).dayofyear\n",
    "\n",
    "            for day in df_year.index[6:]:\n",
    "                if df_year[\"date\"].min() == (day - pd.to_timedelta(6, unit=\"D\")):\n",
    "                    df_year.loc[day, \"weekly_sap\"] = df_year.loc[day][\"cum_sap\"]\n",
    "                    df_year.loc[day, \"weekly_sugarwt\"] = df_year.loc[day][\"cum_sugarwt\"]\n",
    "                else:\n",
    "                    df_year.loc[day, \"weekly_sap\"] = (\n",
    "                        df_year.loc[day][\"cum_sap\"]\n",
    "                        - df_year.loc[day - pd.to_timedelta(7, unit=\"D\")][\"cum_sap\"]\n",
    "                    )\n",
    "                    df_year.loc[day, \"weekly_sugarwt\"] = (\n",
    "                        df_year.loc[day][\"cum_sugarwt\"]\n",
    "                        - df_year.loc[day - pd.to_timedelta(7, unit=\"D\")][\"cum_sugarwt\"]\n",
    "                    )\n",
    "                    df_year.loc[day, \"cum_syrupLitres\"] = (\n",
    "                        df_year.loc[day, \"cum_sugarwt\"] / 1.33\n",
    "                    )\n",
    "                    df_year.loc[day, \"weekly_syrupLitres\"] = (\n",
    "                        df_year.loc[day, \"weekly_sugarwt\"] / 1.33\n",
    "                    )\n",
    "\n",
    "            df_year[\"date_from\"] = df_year[\"date\"] - pd.to_timedelta(6, unit=\"D\")\n",
    "            df_year[\"date_to\"] = df_year[\"date\"]\n",
    "            df_year[\"jd_from\"] = df_year[\"jd\"] - 6\n",
    "            df_year[\"jd_to\"] = df_year[\"jd\"]\n",
    "            df_year = df_year.drop(columns=[\"date\", \"jd\", \"record_id\"])\n",
    "\n",
    "            weekly_df = weekly_df.append(df_year)\n",
    "\n",
    "    return weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_ids(stinson2019) # Add unique record id column to dataframe\n",
    "normalized_data = normalized_tables(data) # Move data to normalized tables\n",
    "full_df = get_weekly_data(normalized_data) # Calculate weekly summary parameters\n",
    "full_df = full_df.reset_index().merge(normalized_data['site'], on='tree', how='left')\n",
    "sap_sugar_df = full_df.loc[:,['date_from', 'date_to', 'weekly_sugarwt', 'weekly_sap','site']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_pickle('../data/stinson2019/full_weekly_summary')\n",
    "sap_sugar_df.to_pickle('../data/stinson2019/sap_sugar_weekly_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
